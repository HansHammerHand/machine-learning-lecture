{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to CTAO Data and Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The story so far:\n",
    "\n",
    "- Linear Discriminant Analysis (LDA) and Fisher's linear discriminant\n",
    "- Principal Component Analysis (PCA)\n",
    "- Feature Selection\n",
    "- Supervised Learning\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "plots.set_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Complete Example\n",
    "\n",
    "Below we load a dataset containing data from simulated CTA Observations.\n",
    "\n",
    "<img width=\"100%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/CTA_Telescopes_in_Southern_Hemisphere.jpg/1280px-CTA_Telescopes_in_Southern_Hemisphere.jpg\" />   \n",
    "\n",
    "We will perform the typical steps to build and evaluate a classifier.\n",
    "\n",
    "0. Understand where your data comes from\n",
    "\n",
    "1. Preprocessing\n",
    "    * Drop Constant Values,\n",
    "    * Handle Missing Data \n",
    "    * Feature Generation\n",
    "\n",
    "2. Splitting\n",
    "    \n",
    "    * Split your data into training and evaluation sets\n",
    "    \n",
    "3. Training \n",
    "    \n",
    "    * Train your classifier of choice.\n",
    "    \n",
    "4. Evaluation\n",
    "    \n",
    "    * Evaluate the performance on the test data set.\n",
    "    * If not good enough, go back to step 1 \n",
    "    \n",
    "5. Physics\n",
    "    \n",
    "    * Check whether your data support your hypothesis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get to know your data\n",
    "\n",
    "Cherenkov telescopes record short flashes of light produced by very high energy cosmic rays and photons hitting earths atmosphere.\n",
    "\n",
    "![](https://live.staticflickr.com/65535/45643242354_79720d6499_k_d.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<!-- https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download -->\n",
    "<video width=\"100%\" controls>\n",
    "  <source src=\"https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use machine learning for two tasks in this example. \n",
    "\n",
    " * Train a classifier to distinguish events induced by gamma rays form events induced by cosmic rays\n",
    " * Train a regressor to estimate the energy of the incoming primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data\n",
    "\n",
    "A _**lot**_ of preprocessing has _already_ happened at this point.\n",
    "\n",
    "* Calibration of Raw Data\n",
    "* Data Reduction from voltage timeseries per pixel to number of photons and mean time for each pixel\n",
    "* Calculation of image features\n",
    "* Geometrical Reconstruction of the Shower Geometry\n",
    "\n",
    "\n",
    "Load data and remove unwanted columns and store the true labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ctapipe.io import TableLoader\n",
    "from ctapipe.utils import get_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset here is very similar but much smaller than the full dataset released publicly here:\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7298569.svg)](https://doi.org/10.5281/zenodo.7298569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_path = get_dataset_path(\"gamma_diffuse_dl2_train_small.dl2.h5\")\n",
    "proton_path = get_dataset_path(\"proton_dl2_train_small.dl2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TableLoader(gamma_path) as loader:\n",
    "    subarray = loader.subarray\n",
    "\n",
    "subarray.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_events(path):\n",
    "\n",
    "    loader = TableLoader(\n",
    "        path,\n",
    "        dl2=True,\n",
    "        instrument=True,\n",
    "        simulated=True,\n",
    "    )\n",
    "\n",
    "    table = loader.read_telescope_events()\n",
    "\n",
    "    # these two columns are arrays in each row, which is not supported by pandas\n",
    "    table.remove_columns([\"tels_with_trigger\", \"HillasReconstructor_telescopes\"])\n",
    "\n",
    "    # convert astropy.table.Table to pd.DataFrame\n",
    "    return table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = read_events(gamma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gammas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete all simulated values which can not be observed during measurement in the physical world. We know which columns to remove because they have a special prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_columns = \"true_|obs_id|event_id\"\n",
    "gammas = gammas.filter(regex=f\"^(?!{forbidden_columns}).*$\")\n",
    "\n",
    "len(gammas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data types of the columns. We can select non-numeric types and encode them. But in this case we might as well drop them as the attribute is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = gammas.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "print(\"Removed columns:\", c.values)\n",
    "\n",
    "gammas = gammas.drop(c, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot the columns with constant values by looking at the count and/or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = gammas.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = desc.columns[desc.loc[\"std\"] == 0]\n",
    "print(\"Removed columns:\", c.values)\n",
    "gammas = gammas.drop(c, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns where all rows are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = gammas.columns[gammas.count() == 0]\n",
    "print(\"Removed columns:\", c.values)\n",
    "gammas = gammas.drop(c, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we do a specific pre-selection, again using \"expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gammas))\n",
    "gammas = gammas[gammas[\"hillas_width\"] > 0]\n",
    "print(len(gammas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data. (Just delete it in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gammas))\n",
    "gammas = gammas.dropna()\n",
    "print(len(gammas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only loaded simulated gamma-ray showers. Now we do the same for the cosmic ray events. We create a method to perform all preprocessing in one step. We need this several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.filter(regex=f\"^(?!{forbidden_columns}).*$\")\n",
    "\n",
    "    c = df.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "    df = df.drop(c, axis=\"columns\")\n",
    "\n",
    "    c = df.columns[df.count() == 0]\n",
    "    df = df.drop(c, axis=\"columns\")\n",
    "\n",
    "    desc = df.describe()\n",
    "\n",
    "    c = desc.columns[desc.loc[\"std\"] == 0]\n",
    "    df = df.drop(c, axis=\"columns\")\n",
    "\n",
    "    df = df[df[\"hillas_width\"] > 0]\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = read_events(gamma_path)\n",
    "gammas = preprocess(gammas)\n",
    "\n",
    "protons = read_events(proton_path)\n",
    "protons = preprocess(protons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform feature generation. We use our expert knowledge or intuition to create a new feature by combining existing columns into a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generation(df):\n",
    "    df[\"awesome_feature\"] = df.eval(\"hillas_intensity / (hillas_width * hillas_length)\")\n",
    "\n",
    "    # distance of impact point to the telescope\n",
    "    df[\"impact\"] = np.sqrt(\n",
    "        (df[\"HillasReconstructor_core_x\"] - df[\"pos_x\"]) ** 2\n",
    "        + (df[\"HillasReconstructor_core_y\"] - df[\"pos_y\"]) ** 2\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "gammas = feature_generation(gammas)\n",
    "protons = feature_generation(protons)\n",
    "\n",
    "gammas[[\"awesome_feature\", \"impact\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the data so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins = np.geomspace(0.01, 1, 101)\n",
    "# bins = np.logspace(0, 1, 100)\n",
    "# bins = 100\n",
    "# bins = np.arange(0, 10) - 0.5\n",
    "bins = np.geomspace(1e3, 1e5, 51)\n",
    "\n",
    "col = \"awesome_feature\"\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(gammas[col], bins=bins, histtype=\"step\", lw=2, label=\"Gammas\", density=True)\n",
    "plt.hist(protons[col], bins=bins, histtype=\"step\", lw=2, label=\"Protons\", density=True)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.xlabel(col)\n",
    "plt.legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we combine the two datasets into one big matrix and build a label vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([gammas, protons])\n",
    "y = np.concatenate([np.ones(len(gammas)), np.zeros(len(protons))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data\n",
    "\n",
    "Now we can split the data into test and training sets. Scikit-Learn provides some neat methods to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the classifier\n",
    "\n",
    "Now we can train any classifier we want on the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=15, criterion=\"entropy\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation \n",
    "\n",
    "Check accuracy of the models and other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.Series(rf.feature_importances_, index=gammas.columns)\n",
    "\n",
    "plt.figure()\n",
    "importance.sort_values().tail(20).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, thresholds):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(fpr, tpr, \"--\", color=\"gray\", alpha=0.5)\n",
    "    plot = ax.scatter(fpr, tpr, c=thresholds, vmax=1)\n",
    "    fig.colorbar(plot)\n",
    "    ax.text(0.5, 0.5, f\"AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}\")\n",
    "\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform steps 3, 4, and 5 in one step using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=12, criterion=\"entropy\")\n",
    "\n",
    "scoring = {\"acc\": \"accuracy\", \"auc\": \"roc_auc\", \"recall\": \"recall\"}\n",
    "\n",
    "results = cross_validate(rf, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = results[\"test_auc\"]\n",
    "recall = results[\"test_recall\"]\n",
    "acc = results[\"test_acc\"]\n",
    "\n",
    "print(f\"Area under RoC curve: {auc.mean():0.04f} ± {auc.std():0.04f}\")\n",
    "print(f\"Accuracy:             {acc.mean():0.04f} ± {acc.std():0.04f}\")\n",
    "print(f\"Recall:               {recall.mean():0.04f} ± {recall.std():0.04f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Physics\n",
    "\n",
    "Now we could test our model and our hypothesis on real observed data. This part of the analysis is the most time \n",
    "consuming in general. It also requires more data than than this notebook can handle. \n",
    "After careful analysis one can produce an image of the gamma-ray sky\n",
    "\n",
    "<img width=\"60%\" src=\"https://www.mpi-hd.mpg.de/hfm/HESS/hgps/figures/HESS_J1813m126.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Improving Classification\n",
    "\n",
    "\n",
    "### Boosting and AdaBoost\n",
    "\n",
    "Similar to the idea of combining many classifiers through bagging (like we did for the RandomForests) we now \n",
    "train many estimators in a sequential manner. In each iteration the data gets modified slightly using weights $w$\n",
    "for each sample in the training data. In the first iteration the weights are all set to $w=1$\n",
    "\n",
    "In each successive iteration the weights are updated. The samples that were incorrectly classified in the previous \n",
    "iteration get a higher weight. The weights for correctly classified samples get decreases. \n",
    "In other words: We increase the influence/importance of samples that are difficult to classify.\n",
    "\n",
    "Predictions are performed by taking a weighted average of the single predictors.\n",
    "\n",
    "The popular AdaBoost algorithms takes this a step further by optimizing the weight of each separate classifier \n",
    "in the ensemble.\n",
    "The AdaBoost ensemble combines many learners in an iterative way. The learner at iteration $m$ is:\n",
    "\n",
    "$$\n",
    " F_{m}(x)=F_{m-1}(x)+\\gamma _{m}h_{m}(x)\n",
    "$$\n",
    "\n",
    "The choice of $F_0$ is problem specific.\n",
    "\n",
    "Each weak learner produces a prediction $h(x_{m})$ for each sample in the training set. At each iteration $m$ a \n",
    "weak learner is fitted and assigned a coefficient $\\gamma_{m}$ which is found by minimizing:\n",
    "\n",
    "$$\n",
    "\\gamma_m = {\\underset {\\gamma }{\\arg \\min }} \\sum_{i}^{N}E\\bigl(F_{m-1}(x_{i})+\\gamma h(x_{i})\\bigr)\n",
    "$$\n",
    "\n",
    "where $E(F)$ is some error function and $x_i$ is the reweighted data sample.\n",
    "\n",
    "In general this method can work with any classifying method. Traditionally it is being used with very small \n",
    "decision trees. \n",
    "The weights get used to select the split points during the minimization of the loss function in each node\n",
    "\n",
    "$$\n",
    " \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} IG(X,Y) =   \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} ( H(Y) - H(Y |\\, X) ).\n",
    "$$\n",
    "\n",
    "Below we try AdaBoost on the CTA data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    ")\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = ada.predict(X_test)\n",
    "y_prediction_proba = ada.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(list(ada.staged_score(X_test, y_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scores, \".\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "\n",
    "Very similar to AdaBoost. Only this time we change the target label we train the classifiers for.\n",
    "\n",
    "Formulate the general problem as follows (See Wikipedia):\n",
    "\n",
    "Starts with a constant function $F_{0}(x)$ and some differentiable loss function $L$ and incrementally expands it in a greedy fashion:\n",
    "\n",
    "$$\n",
    "F_{0}(x)={\\underset {\\gamma }{\\arg \\min }}{\\sum _{i=1}^{n}{L(y_{i},\\gamma )}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{m}(x)=F_{m-1}(x)+{\\underset {h_{m}\\in {\\mathcal {H}}}{\\operatorname {arg\\,min} }}\\left[{\\sum _{i=1}^{n}{L(y_{i},F_{m-1}(x_{i})+h_{m}(x_{i}))}}\\right]\n",
    "$$\n",
    "\n",
    "Finding the best $ h_{m}\\in {\\mathcal {H}}$ is computationally speaking impossible.\n",
    "If we could find the perfect $h$ however, we know that \n",
    "\n",
    "$$\n",
    "F_{m+1}(x_i)=F_{m}(x_i)+h(x_i)=y_i\n",
    "$$\n",
    "\n",
    "or, equivalently, \n",
    "\n",
    "$$\n",
    "   h(x_i)= y_i - F_{m}(x_i)\n",
    "$$\n",
    "\n",
    "Note that for the mean squared error loss $\\frac{1}{2}(y_i - F(x_i))^2$ this is equivalent to the negative \n",
    "gradient with respect to $F_i$.\n",
    "\n",
    "For a general loss function we fit $h_{m}(x)$ to the residuals, or negative gradients \n",
    "$$\n",
    " r_{i, m}=-\\left[{\\frac {\\partial L(y_{i},F(x_{i}))}{\\partial F(x_{i})}}\\right]_{F(x)=F_{m-1}(x)}\\quad {\\mbox{for }}i=1,\\ldots ,n.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Below we try it on CTA data again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grb = GradientBoostingClassifier(\n",
    "    verbose=True,\n",
    "    n_estimators=300,\n",
    ")\n",
    "grb.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = grb.predict(X_test)\n",
    "y_prediction_proba = grb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [accuracy_score(y_pred, y_test) for y_pred in grb.staged_predict(X_test)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(l)), l, \".\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "\n",
    "plt.text(0.5, 0.5, f\"AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on gradient descent algorithms can be found in the Neural Network lecture.\n",
    "\n",
    "Let's now test our all time favorite classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=18, criterion=\"entropy\")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "plt.text(0.5, 0.5, f\"AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}\")\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
