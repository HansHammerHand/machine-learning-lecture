{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not delete (math definitions here)\n",
    "\n",
    "$$\n",
    "% vectors and matrices\n",
    "\\def\\v#1{\\boldsymbol{#1}}\n",
    "\\def\\m#1{\\boldsymbol{#1}}\n",
    "\\def\\p#1{\\mathtt{#1}}\n",
    "% confusion matrix stuff\n",
    "\\def\\TP{\\mathit{TP}}\n",
    "\\def\\TN{\\mathit{TN}}\n",
    "\\def\\FP{\\mathit{FP}}\n",
    "\\def\\FN{\\mathit{FN}}\n",
    "% statistics \n",
    "\\def\\mse{\\operatorname{mse}}\n",
    "\\def\\E{\\operatorname{E}}\n",
    "\\def\\Var{\\operatorname{Var}}\n",
    "\\def\\Cov{\\operatorname{Cov}}\n",
    "\\def\\Bias{\\operatorname{Bias}}\n",
    "\\def\\argmax{\\operatorname{arg\\,max}}\n",
    "\\def\\argmin{\\operatorname{arg\\,min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to CTAO Data and Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The story so far:\n",
    "\n",
    "- Linear Discriminant Analysis (LDA) and Fisher's linear discriminant\n",
    "- Principal Component Analysis (PCA)\n",
    "- Feature Selection\n",
    "- Supervised Learning\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:05:27.643365Z",
     "start_time": "2018-11-27T15:05:26.177785Z"
    }
   },
   "outputs": [],
   "source": [
    "from ml import plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:05:27.643365Z",
     "start_time": "2018-11-27T15:05:26.177785Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "plots.set_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Complete Example\n",
    "\n",
    "Below we load a dataset containing data from simulated CTA Observations.\n",
    "\n",
    "<img width=\"100%\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/CTA_Telescopes_in_Southern_Hemisphere.jpg/1280px-CTA_Telescopes_in_Southern_Hemisphere.jpg\" />   \n",
    "\n",
    "We will perform the typical steps to build and evaluate a classifier.\n",
    "\n",
    "0. Understand where your data comes from\n",
    "\n",
    "1. Preprocessing\n",
    "    * Drop Constant Values,\n",
    "    * Handle Missing Data \n",
    "    * Feature Generation\n",
    "\n",
    "2. Splitting\n",
    "    \n",
    "    * Split your data into training and evaluation sets\n",
    "    \n",
    "3. Training \n",
    "    \n",
    "    * Train your classifier of choice.\n",
    "    \n",
    "4. Evaluation\n",
    "    \n",
    "    * Evaluate the performance on the test data set.\n",
    "    * If not good enough, go back to step 1 \n",
    "    \n",
    "5. Physics\n",
    "    \n",
    "    * Check whether your data support your hypothesis\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get to know your data\n",
    "\n",
    "Cherenkov telescopes record short flashes of light produced by very high energy cosmic rays and photons hitting earths atmosphere.\n",
    "\n",
    "![](https://live.staticflickr.com/65535/45643242354_79720d6499_k_d.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:00.589316Z",
     "start_time": "2018-11-27T15:07:00.584438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download -->\n",
       "<video width=\"100%\" controls>\n",
       "  <source src=\"https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<!-- https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download -->\n",
    "<video width=\"100%\" controls>\n",
    "  <source src=\"https://nextcloud.e5.physik.tu-dortmund.de/index.php/s/e7yb2mifGDeyDBN/download\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use machine learning for two tasks in this example. \n",
    "\n",
    " * Train a classifier to distinguish events induced by gamma rays form events induced by cosmic rays\n",
    " * Train a regressor to estimate the energy of the incoming primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess data\n",
    "\n",
    "A _**lot**_ of preprocessing has _already_ happened at this point.\n",
    "\n",
    "* Calibration of Raw Data\n",
    "* Data Reduction from voltage timeseries per pixel to number of photons and mean time for each pixel\n",
    "* Calculation of image features\n",
    "* Geometrical Reconstruction of the Shower Geometry\n",
    "\n",
    "\n",
    "Load data and remove unwanted columns and store the true labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:01.600004Z",
     "start_time": "2018-11-27T15:07:00.592824Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ctapipe.io import TableLoader\n",
    "from ctapipe.utils import get_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset here is very similar but much smaller than the full dataset released publicly here:\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7298569.svg)](https://doi.org/10.5281/zenodo.7298569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "gamma_path = Path(get_dataset_path('gamma_diffuse_dl2_train_small.dl2.h5'))\n",
    "proton_path = Path(get_dataset_path('proton_dl2_train_small.dl2.h5'))\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Gamma file exists: {gamma_path.exists()}\")\n",
    "print(f\"Gamma file path: {gamma_path}\")\n",
    "print(f\"Proton file exists: {proton_path.exists()}\")\n",
    "print(f\"Proton file path: {proton_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TraitError",
     "evalue": "The 'input_url' trait of a TableLoader instance expected a pathlib.Path or non-empty str for an existing file or None, not the str 'C:\\\\Users\\\\Busch\\\\.cache\\\\ctapipe\\\\cccta-dataserver.in2p3.fr\\\\data\\\\ctapipe-test-data\\\\v1.1.0\\\\gamma_diffuse_dl2_train_small.dl2.h5'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTraitError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mTableLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m loader:\n\u001b[32m      2\u001b[39m     subarray = loader.subarray\n\u001b[32m      4\u001b[39m subarray.peek()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\ctapipe\\io\\tableloader.py:231\u001b[39m, in \u001b[36mTableLoader.__init__\u001b[39m\u001b[34m(self, input_url, h5file, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_url \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;28;01mNone\u001b[39;00m, traits.Undefined}:\n\u001b[32m    229\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33minput_url\u001b[39m\u001b[33m\"\u001b[39m] = input_url\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m h5file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNeed to specify either input_url or h5file\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\ctapipe\\core\\component.py:164\u001b[39m, in \u001b[36mComponent.__init__\u001b[39m\u001b[34m(self, config, parent, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    163\u001b[39m         parent = weakref.proxy(parent)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUserWarning\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TraitError(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\config\\configurable.py:92\u001b[39m, in \u001b[36mConfigurable.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m config = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# load kwarg traits, other than config\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# record traits set by config\u001b[39;00m\n\u001b[32m     95\u001b[39m config_override_names = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\traitlets.py:1355\u001b[39m, in \u001b[36mHasTraits.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items():\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_trait(key):\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m         \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1356\u001b[39m         changes[key] = Bunch(\n\u001b[32m   1357\u001b[39m             name=key,\n\u001b[32m   1358\u001b[39m             old=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1361\u001b[39m             \u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mchange\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1362\u001b[39m         )\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1364\u001b[39m         \u001b[38;5;66;03m# passthrough args that don't set traits to super\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\traitlets.py:716\u001b[39m, in \u001b[36mTraitType.__set__\u001b[39m\u001b[34m(self, obj, value)\u001b[39m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_only:\n\u001b[32m    715\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TraitError(\u001b[33m'\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m trait is read-only.\u001b[39m\u001b[33m'\u001b[39m % \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\traitlets.py:690\u001b[39m, in \u001b[36mTraitType.set\u001b[39m\u001b[34m(self, obj, value)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: HasTraits, value: S) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     new_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\traitlets.py:722\u001b[39m, in \u001b[36mTraitType._validate\u001b[39m\u001b[34m(self, obj, value)\u001b[39m\n\u001b[32m    720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalidate\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj._cross_validation_lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    724\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._cross_validate(obj, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\ctapipe\\core\\traits.py:236\u001b[39m, in \u001b[36mPath.validate\u001b[39m\u001b[34m(self, obj, value)\u001b[39m\n\u001b[32m    234\u001b[39m         value = pathlib.Path(url.netloc, url.path)\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m value = value.absolute()\n\u001b[32m    239\u001b[39m exists = value.exists()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Busch\\anaconda3\\envs\\ml\\Lib\\site-packages\\traitlets\\traitlets.py:831\u001b[39m, in \u001b[36mTraitType.error\u001b[39m\u001b[34m(self, obj, value, error, info)\u001b[39m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    826\u001b[39m     e = \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m trait expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, not \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    827\u001b[39m         \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    828\u001b[39m         info \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info(),\n\u001b[32m    829\u001b[39m         describe(\u001b[33m\"\u001b[39m\u001b[33mthe\u001b[39m\u001b[33m\"\u001b[39m, value),\n\u001b[32m    830\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TraitError(e)\n",
      "\u001b[31mTraitError\u001b[39m: The 'input_url' trait of a TableLoader instance expected a pathlib.Path or non-empty str for an existing file or None, not the str 'C:\\\\Users\\\\Busch\\\\.cache\\\\ctapipe\\\\cccta-dataserver.in2p3.fr\\\\data\\\\ctapipe-test-data\\\\v1.1.0\\\\gamma_diffuse_dl2_train_small.dl2.h5'."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with TableLoader(gamma_path) as loader:\n",
    "        subarray = loader.subarray\n",
    "    \n",
    "    subarray.peek()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading with Path object: {e}\")\n",
    "    print(\"Trying with string path...\")\n",
    "    \n",
    "    # Fallback: try with string path\n",
    "    try:\n",
    "        with TableLoader(str(gamma_path)) as loader:\n",
    "            subarray = loader.subarray\n",
    "        \n",
    "        subarray.peek()\n",
    "    except Exception as e2:\n",
    "        print(f\"Error loading with string path: {e2}\")\n",
    "        print(\"You may need to download the test data first.\")\n",
    "        print(\"Try running: ctapipe-download-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.004702Z",
     "start_time": "2018-11-27T15:07:01.603010Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_events(path):\n",
    "\n",
    "    loader = TableLoader(\n",
    "        path,\n",
    "        dl2=True,\n",
    "        instrument=True,\n",
    "        simulated=True,\n",
    "    )\n",
    "\n",
    "    table = loader.read_telescope_events()\n",
    "\n",
    "    # these two columns are arrays in each row, which is not supported by pandas\n",
    "    table.remove_columns(['tels_with_trigger', 'HillasReconstructor_telescopes'])\n",
    "\n",
    "    # convert astropy.table.Table to pd.DataFrame\n",
    "    return table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = read_events(gamma_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gammas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete all simulated values which can not be observed during measurement in the physical world. We know which columns to remove because they have a special prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.055025Z",
     "start_time": "2018-11-27T15:07:02.007914Z"
    }
   },
   "outputs": [],
   "source": [
    "forbidden_columns = 'true_|obs_id|event_id'\n",
    "gammas = gammas.filter(regex=f'^(?!{forbidden_columns}).*$')\n",
    "\n",
    "len(gammas.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data types of the columns. We can select non-numeric types and encode them. But in this case we might as well drop them as the attribute is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.075296Z",
     "start_time": "2018-11-27T15:07:02.057255Z"
    }
   },
   "outputs": [],
   "source": [
    "c = gammas.select_dtypes(exclude=['number', 'bool']).columns\n",
    "print('Removed columns:', c.values)\n",
    "\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot the columns with constant values by looking at the count and/or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.529060Z",
     "start_time": "2018-11-27T15:07:02.078743Z"
    }
   },
   "outputs": [],
   "source": [
    "desc = gammas.describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.547778Z",
     "start_time": "2018-11-27T15:07:02.532415Z"
    }
   },
   "outputs": [],
   "source": [
    "c = desc.columns[desc.loc['std'] == 0]\n",
    "print('Removed columns:', c.values)\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns where all rows are nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = gammas.columns[gammas.count() == 0]\n",
    "print('Removed columns:', c.values)\n",
    "gammas = gammas.drop(c, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we do a specific pre-selection, again using \"expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gammas))\n",
    "gammas = gammas[gammas['hillas_width'] > 0]\n",
    "print(len(gammas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing data. (Just delete it in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.594941Z",
     "start_time": "2018-11-27T15:07:02.551401Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(gammas))\n",
    "gammas = gammas.dropna()\n",
    "print(len(gammas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only loaded simulated gamma-ray showers. Now we do the same for the cosmic ray events. We create a method to perform all preprocessing in one step. We need this several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:02.608429Z",
     "start_time": "2018-11-27T15:07:02.597366Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.filter(regex=f'^(?!{forbidden_columns}).*$')\n",
    "    \n",
    "    c = df.select_dtypes(exclude=['number', 'bool']).columns\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    c = df.columns[df.count() == 0]\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    desc = df.describe()\n",
    "    \n",
    "    c = desc.columns[desc.loc['std'] == 0]\n",
    "    df = df.drop(c, axis='columns')\n",
    "    \n",
    "    df = df[df['hillas_width'] > 0]\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.467654Z",
     "start_time": "2018-11-27T15:07:02.611649Z"
    }
   },
   "outputs": [],
   "source": [
    "gammas = read_events(gamma_path)\n",
    "gammas = preprocess(gammas)\n",
    "\n",
    "protons = read_events(proton_path)\n",
    "protons = preprocess(protons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform feature generation. We use our expert knowledge or intuition to create a new feature by combining existing columns into a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.481076Z",
     "start_time": "2018-11-27T15:07:03.471131Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_generation(df):\n",
    "    df['awesome_feature'] =  df.eval('hillas_intensity / (hillas_width * hillas_length)')\n",
    "    \n",
    "    # distance of impact point to the telescope\n",
    "    df['impact'] = np.sqrt(\n",
    "        (df['HillasReconstructor_core_x'] - df['pos_x'])**2\n",
    "        + (df['HillasReconstructor_core_y'] - df['pos_y'])**2\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "gammas = feature_generation(gammas)\n",
    "protons = feature_generation(protons)\n",
    "\n",
    "gammas[['awesome_feature', 'impact']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the data so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.829801Z",
     "start_time": "2018-11-27T15:07:03.484278Z"
    }
   },
   "outputs": [],
   "source": [
    "# bins = np.geomspace(0.01, 1, 101)\n",
    "# bins = np.logspace(0, 1, 100)\n",
    "# bins = 100\n",
    "# bins = np.arange(0, 10) - 0.5\n",
    "bins = np.geomspace(1e3, 1e5, 51)\n",
    "\n",
    "col = 'awesome_feature'\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(gammas[col], bins=bins, histtype='step', lw=2, label='Gammas', density=True)\n",
    "plt.hist(protons[col], bins=bins, histtype='step', lw=2, label='Protons', density=True)\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel(col)\n",
    "plt.legend()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we combine the two datasets into one big matrix and build a label vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.895879Z",
     "start_time": "2018-11-27T15:07:03.832960Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.concat([gammas, protons])\n",
    "y = np.concatenate([np.ones(len(gammas)), np.zeros(len(protons))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data\n",
    "\n",
    "Now we can split the data into test and training sets. Scikit-Learn provides some neat methods to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:07:03.983514Z",
     "start_time": "2018-11-27T15:07:03.898708Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_test, X_train, y_test, y_train = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the classifier\n",
    "\n",
    "Now we can train any classifier we want on the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:04.181685Z",
     "start_time": "2018-11-27T15:13:02.875532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=15, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation \n",
    "\n",
    "Check accuracy of the models and other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.Series(rf.feature_importances_, index=gammas.columns)\n",
    "\n",
    "plt.figure()\n",
    "importance.sort_values().tail(20).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:04.199205Z",
     "start_time": "2018-11-27T15:13:04.183884Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:13:46.845086Z",
     "start_time": "2018-11-27T15:13:46.501506Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, thresholds):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(fpr, tpr, '--', color='gray', alpha=0.5)\n",
    "    plot = ax.scatter(fpr, tpr, c=thresholds, vmax=1)\n",
    "    fig.colorbar(plot)\n",
    "    ax.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "\n",
    "\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    \n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform steps 3, 4, and 5 in one step using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:08:28.709336Z",
     "start_time": "2018-11-27T15:08:03.738680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rf = DecisionTreeClassifier(max_depth=12, criterion='entropy')\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'auc': 'roc_auc',\n",
    "           'recall': 'recall'}\n",
    "\n",
    "results = cross_validate(rf, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:12:14.425262Z",
     "start_time": "2018-11-27T15:12:14.404548Z"
    }
   },
   "outputs": [],
   "source": [
    "auc = results['test_auc']\n",
    "recall = results['test_recall']\n",
    "acc = results['test_acc']\n",
    "\n",
    "print(f'Area under RoC curve: {auc.mean():0.04f} ± {auc.std():0.04f}')\n",
    "print(f'Accuracy:             {acc.mean():0.04f} ± {acc.std():0.04f}')\n",
    "print(f'Recall:               {recall.mean():0.04f} ± {recall.std():0.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Physics\n",
    "\n",
    "Now we could test our model and our hypothesis on real observed data. This part of the analysis is the most time \n",
    "consuming in general. It also requires more data than than this notebook can handle. \n",
    "After careful analysis one can produce an image of the gamma-ray sky\n",
    "\n",
    "<img width=\"60%\" src=\"https://www.mpi-hd.mpg.de/hfm/HESS/hgps/figures/HESS_J1813m126.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Improving Classification\n",
    "\n",
    "\n",
    "### Boosting and AdaBoost\n",
    "\n",
    "Similar to the idea of combining many classifiers through bagging (like we did for the RandomForests) we now \n",
    "train many estimators in a sequential manner. In each iteration the data gets modified slightly using weights $w$\n",
    "for each sample in the training data. In the first iteration the weights are all set to $w=1$\n",
    "\n",
    "In each successive iteration the weights are updated. The samples that were incorrectly classified in the previous \n",
    "iteration get a higher weight. The weights for correctly classified samples get decreases. \n",
    "In other words: We increase the influence/importance of samples that are difficult to classify.\n",
    "\n",
    "Predictions are performed by taking a weighted average of the single predictors.\n",
    "\n",
    "The popular AdaBoost algorithms takes this a step further by optimizing the weight of each separate classifier \n",
    "in the ensemble.\n",
    "The AdaBoost ensemble combines many learners in an iterative way. The learner at iteration $m$ is:\n",
    "\n",
    "$$\n",
    " F_{m}(x)=F_{m-1}(x)+\\gamma _{m}h_{m}(x)\n",
    "$$\n",
    "\n",
    "The choice of $F_0$ is problem specific.\n",
    "\n",
    "Each weak learner produces a prediction $h(x_{m})$ for each sample in the training set. At each iteration $m$ a \n",
    "weak learner is fitted and assigned a coefficient $\\gamma_{m}$ which is found by minimizing:\n",
    "\n",
    "$$\n",
    "\\gamma_m = {\\underset {\\gamma }{\\arg \\min }} \\sum_{i}^{N}E\\bigl(F_{m-1}(x_{i})+\\gamma h(x_{i})\\bigr)\n",
    "$$\n",
    "\n",
    "where $E(F)$ is some error function and $x_i$ is the reweighted data sample.\n",
    "\n",
    "In general this method can work with any classifying method. Traditionally it is being used with very small \n",
    "decision trees. \n",
    "The weights get used to select the split points during the minimization of the loss function in each node\n",
    "\n",
    "$$\n",
    " \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} IG(X,Y) =   \\underset{(X, s) \\in \\, \\mathbf{X} \\times {S}}{\\arg \\max} ( H(Y) - H(Y |\\, X) ).\n",
    "$$\n",
    "\n",
    "Below we try AdaBoost on the CTA data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:09.247110Z",
     "start_time": "2018-11-27T15:48:58.872812Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    ")\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = ada.predict(X_test)\n",
    "y_prediction_proba = ada.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:10.072785Z",
     "start_time": "2018-11-27T15:49:09.249195Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = np.array(list(ada.staged_score(X_test, y_test)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scores, '.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T15:49:10.727863Z",
     "start_time": "2018-11-27T15:49:10.075262Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting \n",
    "\n",
    "Very similar to AdaBoost. Only this time we change the target label we train the classifiers for.\n",
    "\n",
    "Formulate the general problem as follows (See Wikipedia):\n",
    "\n",
    "Starts with a constant function $F_{0}(x)$ and some differentiable loss function $L$ and incrementally expands it in a greedy fashion:\n",
    "\n",
    "$$\n",
    "F_{0}(x)={\\underset {\\gamma }{\\arg \\min }}{\\sum _{i=1}^{n}{L(y_{i},\\gamma )}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{m}(x)=F_{m-1}(x)+{\\underset {h_{m}\\in {\\mathcal {H}}}{\\operatorname {arg\\,min} }}\\left[{\\sum _{i=1}^{n}{L(y_{i},F_{m-1}(x_{i})+h_{m}(x_{i}))}}\\right]\n",
    "$$\n",
    "\n",
    "Finding the best $ h_{m}\\in {\\mathcal {H}}$ is computationally speaking impossible.\n",
    "If we could find the perfect $h$ however, we know that \n",
    "\n",
    "$$\n",
    "F_{m+1}(x_i)=F_{m}(x_i)+h(x_i)=y_i\n",
    "$$\n",
    "\n",
    "or, equivalently, \n",
    "\n",
    "$$\n",
    "   h(x_i)= y_i - F_{m}(x_i)\n",
    "$$\n",
    "\n",
    "Note that for the mean squared error loss $\\frac{1}{2}(y_i - F(x_i))^2$ this is equivalent to the negative \n",
    "gradient with respect to $F_i$.\n",
    "\n",
    "For a general loss function we fit $h_{m}(x)$ to the residuals, or negative gradients \n",
    "$$\n",
    " r_{i, m}=-\\left[{\\frac {\\partial L(y_{i},F(x_{i}))}{\\partial F(x_{i})}}\\right]_{F(x)=F_{m-1}(x)}\\quad {\\mbox{for }}i=1,\\ldots ,n.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Below we try it on CTA data again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:38.715276Z",
     "start_time": "2018-11-27T16:56:29.657159Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grb = GradientBoostingClassifier(\n",
    "    verbose=True,\n",
    "    n_estimators=300,\n",
    ")\n",
    "grb.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = grb.predict(X_test)\n",
    "y_prediction_proba = grb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:39.343691Z",
     "start_time": "2018-11-27T16:56:38.718176Z"
    }
   },
   "outputs": [],
   "source": [
    "l = [accuracy_score(y_pred, y_test) for y_pred in grb.staged_predict(X_test)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(l)), l, '.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:56:40.215880Z",
     "start_time": "2018-11-27T16:56:39.346460Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on gradient descent algorithms can be found in the Neural Network lecture.\n",
    "\n",
    "Let's now test our all time favorite classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:58:57.873659Z",
     "start_time": "2018-11-27T16:58:45.510336Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150,  max_depth=18, criterion='entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = rf.predict(X_test)\n",
    "y_prediction_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:58:58.442111Z",
     "start_time": "2018-11-27T16:58:57.875736Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_prediction)\n",
    "auc = roc_auc_score(y_test, y_prediction_proba[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prediction_proba[:, 1])\n",
    "\n",
    "plot_roc(fpr, tpr, thresholds)\n",
    "plt.text(0.5, 0.5, f'AuC ROC: {auc:0.03f} \\nAccuracy: {acc:0.03f}')\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
