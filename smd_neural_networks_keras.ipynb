{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Short Introduction to Neural Networks and Deep Learning with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# keras supports tensorflow, torch and jax\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plt.rcParams[\"figure.constrained_layout.use\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# How to define a Neural Network Architecture in Keras\n",
    "\n",
    "To declare a new Network architecture, we create an instance of [`keras.Sequential`](https://keras.io/guides/sequential_model/)\n",
    "\n",
    "We can define the layers that are applied *in sequence*. \n",
    "\n",
    "Keras completely takes care about gradient computation using back propagation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    name=\"fully-connected\",\n",
    "    layers=[\n",
    "        layers.Dense(128, activation=\"relu\", name=\"hidden-1\"),\n",
    "        layers.Dense(128, activation=\"relu\", name=\"hidden-2\"),\n",
    "        layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Observe that the input shapes are not yet fixed. It will be determined once applied to data for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.zeros((1, 8 * 8))\n",
    "model(dummy)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now we are building a more flexible model, where we can pass some options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_classes, n_hidden, dropout=0.25, activation=\"leaky_relu\"):\n",
    "    return keras.Sequential(\n",
    "        layers=[\n",
    "            # flatten and normalize input\n",
    "            layers.Flatten(),\n",
    "            layers.BatchNormalization(),\n",
    "            # first hidden layer\n",
    "            layers.Dense(n_hidden, activation=activation),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout),\n",
    "            # second hidden layer\n",
    "            layers.Dense(n_hidden, activation=activation),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(dropout),\n",
    "            # output layer\n",
    "            layers.Dense(n_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "model = create_model(n_classes=10, n_hidden=128)\n",
    "model(np.zeros((1, 28, 28)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Keras comes with default fit / evaluate functions.\n",
    "\n",
    "We could roll our own, but for this simple examples, we are going to use the defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(9, 3), constrained_layout=True)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(x_train[i], cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Now we need to compile the model, which also defines loss function, optimizer and metrics we want to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_classes=10, n_hidden=128)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    ")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import IndexLocator\n",
    "\n",
    "\n",
    "def plot_losses(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "    ax1.plot(history.history[\"loss\"], label=\"train\")\n",
    "    ax1.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "\n",
    "    ax2.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "    ax2.plot(history.history[\"val_accuracy\"], label=\"validation\")\n",
    "\n",
    "    ax1.set(\n",
    "        ylabel=\"loss\",\n",
    "    )\n",
    "    ax2.set(\n",
    "        xlabel=\"epoch\",\n",
    "        ylabel=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax2.xaxis.set_major_locator(IndexLocator(2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The validation loss is lower in the beginning, mainly due to two reasons:\n",
    "\n",
    "* The model is learning fast and the train loss is the mean over the epoch while the validation loss is evaluated at the end of the epoch.\n",
    "* Dropout is active for the training evaluation, but the validation uses the full network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes = dict(\n",
    "    enumerate(\n",
    "        [\n",
    "            \"airplane\",\n",
    "            \"automobile\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(9, 9), constrained_layout=True)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "indices = rng.choice(len(y_train), size=axs.size)\n",
    "\n",
    "for idx, ax in zip(indices, axs.flat):\n",
    "\n",
    "    img = x_train[idx]\n",
    "\n",
    "    ax.set_title(cifar10_classes[y_train[idx, 0]])\n",
    "    ax.imshow(img)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_classes=len(cifar10_classes), n_hidden=128)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    ")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We do not get much better than 50 % with a fully connected network.\n",
    "\n",
    "Let's try a convolutional network. First a relatively simple one, based on the Keras examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "n_classes = len(cifar10_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(n_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    ")\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Let's try another architecture, from https://arxiv.org/abs/1409.1556\n",
    "\n",
    "> Very Deep Convolutional Networks for Large-Scale Image Recognition  \n",
    "> Karen Simonyan, Andrew Zisserman\n",
    "\n",
    "> In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        # first convolutional stack\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # second convolutional stack\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # third convolutional stack\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # fully-connected part\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"leaky_relu\"),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(n_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_score = model.predict(x_test)\n",
    "prediction = np.argmax(predicted_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "classes = list(cifar10_classes.values())\n",
    "\n",
    "\n",
    "matrix = confusion_matrix(y_test, prediction)\n",
    "matrix = np.divide(matrix, matrix.sum(axis=1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mat = ax.matshow(matrix)\n",
    "ax.set_xticks(np.arange(len(classes)))\n",
    "ax.set_xticklabels(classes, rotation=90)\n",
    "\n",
    "ax.set_yticks(np.arange(len(classes)))\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "fig.colorbar(mat)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_classes = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(9, 9), constrained_layout=True)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "indices = rng.choice(len(y_train), size=axs.size)\n",
    "\n",
    "for idx, ax in zip(indices, axs.flat):\n",
    "\n",
    "    img = x_train[idx]\n",
    "\n",
    "    ax.set_title(fashion_mnist_classes[y_train[idx]])\n",
    "    ax.imshow(img, cmap=\"gray_r\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one dimension for the \"color channels\"\n",
    "\n",
    "x_train = x_train[..., np.newaxis]\n",
    "x_test = x_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "n_classes = len(fashion_mnist_classes)\n",
    "\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        # first convolutional stack\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # second convolutional stack\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # third convolutional stack\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.Conv2D(128, kernel_size=(3, 3), activation=\"leaky_relu\", padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        # fully-connected part\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"leaky_relu\"),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(n_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Further links:\n",
    "* [Keras Documentation](https://keras.io)\n",
    "* [Keras Quickstart Tutorial](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n",
    "* [Keras Examples](https://keras.io/examples/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "The best current performance claimed on CIFAR-10 is 99.5 % accuracy:\n",
    "\n",
    "https://en.wikipedia.org/wiki/CIFAR-10#Research_papers_claiming_state-of-the-art_results_on_CIFAR-10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
