{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml import plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plots.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "Most of this material was heavily inspired by http://cs231n.github.io/. Stanfords CNN for Image Recognition lecture.\n",
    "\n",
    "\n",
    "## Linear Classification II\n",
    "\n",
    "Previously we talked about classification using a linear function $f(x): \\mathbb{R}^{p} \\to \\mathbb{R}$ and least-square regression.\n",
    "\n",
    "Recall the linear function dependent on the parameter vector $\\beta$\n",
    "\n",
    "\n",
    "$$\n",
    "f(x)= \\hat{y} =  \\hat{\\beta}_0 + \\sum_{j=1}^p x_j \\hat{\\beta}_j\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "f(x)= \\hat{y} =  x^T \\mathbf{\\beta}\n",
    "$$\n",
    "\n",
    "where $\\beta = (\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_p)$ and $x = (1, x_1, x_2, \\ldots, x_p)$.\n",
    "\n",
    "We minimized the residual sum of squares using matrix multiplication.\n",
    "\n",
    "$$\n",
    "RSS(\\beta) = (\\mathbf{y} - \\mathbf{X} \\beta)^T (\\mathbf{y} - \\mathbf{X} \\beta )\n",
    "\\Rightarrow \\hat{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "\n",
    "In order to perform the classification we had to define the classification function \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "\\text{Yes}, & \\text{if $ f(x) \\gt 0.5$} \\\\\n",
    "\\text{No}, & \\text{if $ f(x) \\le 0.5$}\n",
    "\\end{cases}\n",
    "$$ \n",
    "\n",
    "Now we encode the class label directly into the function. Given a classification problem with $K$ different classes we want $f(x): \\mathbb{R}^{p} \\to \\mathbb{R}^K$ to return a vector of length $K$. Instead of finding the parameters in a vector $\\beta$ we use a $p \\times K$ matrix $\\mathbf{W}$\n",
    "\n",
    "$$\n",
    "f(x) = \\hat{y} =   x \\cdot \\mathbf{W}\n",
    "$$\n",
    "\n",
    "At this point we could again minimize a modified version of the RSS or the SVM loss. This time however we want to be able to have a more *probabilistic* interpretation of the loss function.\n",
    "\n",
    "We use a new loss function called the __cross-entropy loss__. Also known as logistic loss. Given two probability density functions $p$ and $q$ over the same probabilistic space,  cross entropy is defined as\n",
    "\n",
    "$$\n",
    "H_{\\times}(p, q) =     -\\sum _{x} p(x)\\,\\log q(x).\\!\n",
    "$$\n",
    "\n",
    "The cross entropy gives smaller values for similar distributions $q$ and $p$.\n",
    "In case of classification problems $p$ is the true distribution of classes for a given $x$ and $q(x)$ the estimated distribution as produced by our machine learner. \n",
    "\n",
    "In the example below we look at $p$ for the digits dataset. As one example in the dataset can be just one digit, the $p$ vector has only a single non-zero entry. \n",
    "\n",
    "If a classifier produces the same distribution for the dataset we have perfect classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "y = preprocessing.LabelBinarizer().fit_transform(y)\n",
    "\n",
    "print(\"True (discrete) probability distributions.\")\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 5, figsize=(8, 4))\n",
    "for ax, x_i, y_i in zip(axs.flat, X[10:20], y):\n",
    "    img = x_i.reshape(8, 8)\n",
    "    ax.imshow(img, cmap=\"gray_r\", interpolation=\"nearest\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    print(y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our estimated probabilities for the distribitution of class labels to be as close to the true distribution as possible. The result of our function $f(x)$ does not produce probability estimates. We need to *transform the results to probabilities* using the so called __softmax__ function\n",
    "\n",
    "$$\n",
    "q_k(x) =  \\frac{e^{f_{k}(x)}} {âˆ‘_j e^{f_j(x)}}\n",
    "$$\n",
    "\n",
    "where the index $j \\in \\{1, \\ldots, K \\}$ loops over all classes and $f_k$ is the prediction for class $k$.\n",
    "\n",
    "In shorter words: A classifier using cross entropy loss with softmax produces probabilities for class memberships.\n",
    "\n",
    "Below we implement the softmax function and the loss function which uses a weight matrix $\\mathbf{W}$ to compute the loss of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize as label_binarize_sklearn\n",
    "\n",
    "\n",
    "def label_binarize(labels, n_classes):\n",
    "    # sklearn does not expand binary labels but we need this for our\n",
    "    # formulation of the training\n",
    "    binary_labels = label_binarize_sklearn(labels, classes=range(max(n_classes, 3)))\n",
    "    return binary_labels[:, :n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarize([0, 1, 1, 0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "ones = np.ones(shape=(len(X), 1))\n",
    "X = np.hstack([ones, X])\n",
    "\n",
    "# we have 10 classes and p attributes, digits from 0 to 9 and 64 pixels\n",
    "K = 10\n",
    "p = 64\n",
    "\n",
    "# include the bias into the weight matrix\n",
    "W = rng.normal(size=(p + 1, K))\n",
    "\n",
    "y = label_binarize(y, 10)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function shifted to zero so no numerical instabilities arise.\n",
    "def softmax(f):\n",
    "    f_shifted = f - f.max()\n",
    "    p = np.exp(f_shifted).T / np.sum(np.exp(f_shifted), axis=1)\n",
    "    return p.T\n",
    "\n",
    "\n",
    "# loss for the entire training set\n",
    "def loss_cross_ent(X, y, W):\n",
    "    f = X @ W\n",
    "    q = softmax(f)\n",
    "    l = -np.sum(y * np.log2(q), axis=1)\n",
    "    return l.mean()  # the mean over all samples in the batch/dataset\n",
    "\n",
    "\n",
    "loss_cross_ent(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find those weights in $\\mathbf{W}$ which minimize the loss function. One way to minimize this is to randomly search for matrix entries until we get a better loss function. But thats obviously not very smart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X, y, loss_function, max_iter=10000, rng=None):\n",
    "    rng = rng or np.random.default_rng(0)\n",
    "\n",
    "    # we have 10 classes and p attributes, digits from 0 to 9 and 64 pixels\n",
    "    K = y.shape[1]\n",
    "    p = X.shape[1]\n",
    "\n",
    "    # save all losses\n",
    "    losses = []\n",
    "\n",
    "    # start with a random weight matrix\n",
    "    W = rng.normal(0, 0.01, size=(p, K))\n",
    "    bestloss = np.inf\n",
    "    for i in tqdm(range(max_iter)):\n",
    "\n",
    "        # choose a random direction\n",
    "        W_new = rng.normal(0, 0.01, size=(p, K))\n",
    "        loss = loss_function(X, y, W_new)\n",
    "        losses.append(loss)\n",
    "        if loss < bestloss:\n",
    "            W = W_new\n",
    "            bestloss = loss\n",
    "    return losses\n",
    "\n",
    "\n",
    "max_iter = 5000\n",
    "losses_random = random_search(X, y, loss_cross_ent, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(max_iter), losses_random, \".\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss Function\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in all problems we discussed before, it is actually computationally infeaseble to find the global optimum. We can again try a local search to find a local optimum. Again this is not a show-stopper in real-life situation. Often finding a local optimum suffices. Sometimes the problem can be reformulated to convex in which case the local optimizer is guaranteed to find the global optimum. \n",
    "\n",
    "Below we try an incremental approach. Which optimizes in a local neighbourhood of $\\mathbf{W}$. We keep the best $\\mathbf{W}$ we found in the previous iterations and only add a random direction to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_descent(X, y, loss_function, max_iter=10000, step_size=0.01, rng=None):\n",
    "    rng = rng or np.random.default_rng(0)\n",
    "    bestloss = np.inf\n",
    "\n",
    "    # we have 10 classes and p attributes, digits from 0 to 9 and 64 pixels\n",
    "    K = y.shape[1]\n",
    "    p = X.shape[1]\n",
    "\n",
    "    # save all losses\n",
    "    losses = []\n",
    "\n",
    "    # start with a random weight matrix\n",
    "    W = np.random.normal(size=(p, K)) * step_size\n",
    "    for i in tqdm(range(max_iter)):\n",
    "\n",
    "        # choose a random direction\n",
    "        W_new = rng.normal(W, step_size)\n",
    "\n",
    "        loss = loss_function(X, y, W_new)\n",
    "        losses.append(loss)\n",
    "        if loss < bestloss:\n",
    "            W = W_new\n",
    "            bestloss = loss\n",
    "    return losses\n",
    "\n",
    "\n",
    "max_iter = 2500\n",
    "losses_random = random_descent(X, y, loss_cross_ent, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses_random, \".\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss Function\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each step we shoose a random direction to optimize to. We can be smarter about it by following the gradient of the loss function in each iteration. The loss function will converge much faster. \n",
    "\n",
    "The gradient has to be computed with respect to our weight matrix $\\mathbf{W}$\n",
    "$$\n",
    "L = H_{\\times}(p, q) =     -\\sum _{k} p_k\\,\\log q_k  =  -\\sum _{k} p_k\\,\\log \\left( \\frac{e^{f_{k}(x)}} {âˆ‘_j e^{f_j(x)}} \\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "f = \\mathbf{W} \\cdot x\\, .\n",
    "$$\n",
    "\n",
    "We can build the derivative using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\rm{\\partial} L }{\\rm{\\partial} w} = \\frac{\\rm{\\partial} L }{\\rm{\\partial} q} \\frac{\\rm{\\partial} q }{\\rm{\\partial} f}\\frac{\\rm{\\partial} f }{\\rm{\\partial}w}\n",
    "$$\n",
    "\n",
    "Note that derivatives of vectors always have to take the index into account. \n",
    "Or you can build the Jacobian. \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\rm{\\partial} L }{\\rm{\\partial} f_i} =& - \\sum_k y_k \\frac{\\rm{\\partial} \\log q_k }{\\rm{\\partial} f_i} \\\\\n",
    "=&- \\sum_k y_k \\frac{1}{q_k}\\frac{\\rm{\\partial}q_k }{\\rm{\\partial} f_i} \\\\\n",
    "&\\vdots \\; \\; \\text{Ãœbungsaufgabe} \\\\\n",
    "=& p_i - y_i\n",
    "\\end{align}\n",
    "\n",
    "We use the computed gradient in the code to compute the update to the weight matrix $\\mathbf{W}$\n",
    "\n",
    "\n",
    "        W = W - dW * step_size\n",
    "        \n",
    "The step size, or learning rate controls how far we go down the gradient in each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W, X, y):\n",
    "    f = X @ W\n",
    "    p = softmax(f)\n",
    "    dh = p - y\n",
    "    dW = (X.T @ dh) / dh.shape[0]\n",
    "    return dW\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    X, y, loss_function, gradient, max_iter=10000, step_size=0.01, rng=None\n",
    "):\n",
    "    rng = rng or np.random.default_rng(0)\n",
    "\n",
    "    bestloss = np.inf\n",
    "\n",
    "    # we have 10 classes and p attributes, digits from 0 to 9 and 64 pixels\n",
    "    K = y.shape[1]\n",
    "    p = X.shape[1]\n",
    "\n",
    "    # save all losses\n",
    "    losses = []\n",
    "    # start with a random weight matrix\n",
    "    W = rng.normal(0, step_size, size=(p, K))\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        W = W - step_size * gradient(W, X, y)\n",
    "        loss = loss_function(X, y, W)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "X = np.hstack([ones, X])\n",
    "y = label_binarize(y, 10)\n",
    "\n",
    "losses, W = gradient_descent(X, y, loss_cross_ent, gradient, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses_random, \".\", label=\"random descent\")\n",
    "plt.plot(losses, \".\", label=\"gradient descent\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss Function\")\n",
    "plt.legend()\n",
    "plt.title(\"Cross Entropy Loss\")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses[:200], \".\", label=\"gradient descent\", color=\"orange\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss Function\")\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just implemented the so called SoftMax classifier. This classifier is non-linear.\n",
    "While the simple matrix multiplication $f = \\boldsymbol{X} \\boldsymbol{W}$ is linear , the application of the softmax function certainly is not. Our prediction vector is given by applying a non-linear function to the output of $f$\n",
    "\n",
    "$$\n",
    "p = q(f(x)) = q(\\boldsymbol{x} \\cdot \\boldsymbol{W})\n",
    "$$\n",
    "\n",
    "Below we apply this classifier to some test data. Clearly this on non-linear step does not suffice to classify non-linear data like the two spirals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=1500,\n",
    "    n_features=2,\n",
    "    center_box=(-5, 5),\n",
    "    centers=n_classes,\n",
    "    cluster_std=0.3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# add biases to matrix\n",
    "ones = np.ones(shape=(len(X), 1))\n",
    "X_b = np.hstack([ones, X])\n",
    "y_b = label_binarize(y, n_classes)\n",
    "\n",
    "losses, W = gradient_descent(X_b, y_b, loss_cross_ent, gradient, max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(softmax(X_b @ W), axis=1)\n",
    "\n",
    "\n",
    "cmap = plots.create_discrete_colormap(n_classes)\n",
    "norm = plt.Normalize(-0.5, n_classes - 0.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.axes(aspect=\"equal\")\n",
    "plt.scatter(X[:, 0], X[:, 1], facecolor=\"none\", s=40, edgecolors=cmap(norm(prediction)))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=cmap, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = plots.twospirals(n_samples=300)\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "\n",
    "# add biases to matrix\n",
    "ones = np.ones(shape=(len(X), 1))\n",
    "X_b = np.hstack([ones, X])\n",
    "y_b = label_binarize(y, n_classes)\n",
    "\n",
    "losses, W = gradient_descent(X_b, y_b, loss_cross_ent, gradient, max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = plots.twospirals(n_samples=300)\n",
    "\n",
    "X_b = np.hstack([ones, X])\n",
    "y_b = label_binarize(y, n_classes)\n",
    "\n",
    "prediction = np.argmax(softmax(X_b @ W), axis=1)\n",
    "\n",
    "cmap = plots.create_discrete_colormap(n_classes)\n",
    "norm = plt.Normalize(-0.5, n_classes - 0.5)\n",
    "\n",
    "plt.figure()\n",
    "plt.axes(aspect=1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, s=10, label=\"truth\")\n",
    "plt.scatter(\n",
    "    X[:, 0],\n",
    "    X[:, 1],\n",
    "    facecolor=\"none\",\n",
    "    s=50,\n",
    "    edgecolors=cmap(norm(prediction)),\n",
    "    label=\"prediction\",\n",
    ")\n",
    "plt.legend()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (Multi Layer Perceptron)\n",
    "\n",
    "We use another abstraction to describe the linear classification we used above. \n",
    "We started with an input image of $8 \\times 8$ pixels. We flatten it to get a single vector $x_i$ with $64$ entries.\n",
    "In the next step we multiply the vector with a matrix $W$ with dimensions $64 \\times 10$ and get a result vector of length 10. The following image shows this process in a network like manner. \n",
    "\n",
    "<img src=\"./ml/images/nn.png\">\n",
    "\n",
    "Neural networks use some special slang to describe their architecture. The column of blue dots, i.e. the matrix multiplication, is called a layer. Depending on context this will be called a *hidden layer* or *fully connected layer*. The input and outputs are called the *input* and *output layer* respectively. The white box labeled as Softmax  is called *activation function*\n",
    "\n",
    "\n",
    "<img src=\"./ml/images/nn_wording.png\">\n",
    "\n",
    "\n",
    "We can extend the previous idea further by applying more than one layer of matrix multiplications.\n",
    "\n",
    "$$\n",
    "p = q_2(q_1(x \\cdot W_1), W_2)\n",
    "$$\n",
    "\n",
    "Each entry in the weight matrices $W_1$ and $W_2$ are *learnable* parameters. The matrices have dimension $n_{\\text{in}} \\times n_{\\text{out}}$ (neglecting biases). In the single layer network we trained above we had a single layer with 640 free parameters to optimize. In the case  of two full layers we would have twice as many. \n",
    "\n",
    "\n",
    "<img src=\"./ml/images/nn_two.png\">\n",
    "\n",
    "\n",
    "The method of finding the gradient for such a complex function is actually not different from what we did before. As long as the Loss function is a linear function with respect to the number of samples. This holds for loss functions like the mean square error, the cross entropy loss or the SVM loss. \n",
    "\n",
    "$$\n",
    "L = \\frac 1 N \\sum_n^N L_i\n",
    "$$\n",
    "\n",
    "As long as the activation functions and the loss function  are differentiable the gradient can be calculated either numerically or analytically using __backpropagation__. Which essentially requires nothing more than recursive application of the chain rule of derivatives. In the example above we used backpropagation on a single layer to calculate the gradient. \n",
    "\n",
    "Below we use the sklearn __multi-layer perceptron__ to classify the spiral data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X, y = plots.twospirals(n_samples=400)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=500)\n",
    "clf.fit(X, y)\n",
    "\n",
    "X, y = plots.twospirals(n_samples=400)\n",
    "prediction = clf.predict(X)\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "cmap = plots.create_discrete_colormap(n_classes)\n",
    "norm = plt.Normalize(-0.5, n_classes - 0.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.axes(aspect=1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, s=10, label=\"truth\")\n",
    "plt.scatter(\n",
    "    X[:, 0],\n",
    "    X[:, 1],\n",
    "    facecolor=\"none\",\n",
    "    s=50,\n",
    "    edgecolors=cmap(norm(prediction)),\n",
    "    label=\"prediction\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialized the MLP with two hidden layers of size 10. As our dataset is two dimensional, the input layer has size 2. The two layers are represented as matrices of size $2 \\times 10$ which means we have 40 matrix entries (plus biases) to optimize in total. \n",
    "\n",
    "The more layers and nodes a network has, the higher the representation power. In this case a simple 2 layer network does not suffice to represent the spiral data. \n",
    "\n",
    "\n",
    "http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "fig, [ax1, ax2, ax3, ax4] = plt.subplots(1, 4)\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "last = 0\n",
    "for i in range(10):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 64, 64),\n",
    "        max_iter=i + 1,\n",
    "        random_state=43,\n",
    "        solver=\"sgd\",\n",
    "    )\n",
    "    mlp.fit(X, y)\n",
    "\n",
    "    if i > 0:\n",
    "        plt.suptitle(\"Iteration {}\".format(i + 1))\n",
    "\n",
    "        ax1.set_title(\"Layer 1\")\n",
    "        ax2.set_title(\"Layer 2\")\n",
    "        ax3.set_title(\"Layer 3\")\n",
    "        ax4.set_title(\"Loss Function\")\n",
    "        ax4.set_xlabel(\"Iteration\")\n",
    "\n",
    "        diff_0 = np.asarray(mlp.coefs_[0])\n",
    "        diff_1 = np.asarray(mlp.coefs_[1])\n",
    "        diff_2 = np.asarray(mlp.coefs_[2])\n",
    "        diff_3 = np.asarray(mlp.coefs_[3])\n",
    "\n",
    "        ax1.imshow(\n",
    "            diff_0,\n",
    "        )\n",
    "        ax2.imshow(\n",
    "            diff_1,\n",
    "        )\n",
    "        ax3.imshow(\n",
    "            diff_2,\n",
    "        )\n",
    "        ax4.plot(range(1, i + 2), mlp.loss_curve_, color=\"purple\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "    last_0 = np.asarray(mlp.coefs_[0])\n",
    "    last_1 = np.asarray(mlp.coefs_[1])\n",
    "    last_2 = np.asarray(mlp.coefs_[2])\n",
    "    last_3 = np.asarray(mlp.coefs_[3])\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Activation Functions\n",
    "\n",
    "There are a range of activation functions to choose from. The Softmax functions we used for linear classification is often used in the final/output layer of a network to produce probability estimates.\n",
    "An activation function has to be non-linear. Otherwise layers in the network could be merged together into one single matrix operation. Sometimes activation functions are also called non-linearities\n",
    "\n",
    "##### Sigmoid \n",
    "\n",
    "$$\n",
    "g(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The sigmoid function squashes values into range between 0 and 1.\n",
    "It has been very popular for neural networks in the past but has recently fallen out of favor because it does not behave well when using gradient descent. \n",
    "\n",
    "$$\n",
    "\\rm{d}g(x) = \\frac{e^x}{\\left(1 + e^{x}\\right)^2}\n",
    "$$\n",
    "\n",
    "As visible in the plot below the gradient becomes zero for large values of $x$.  Which leads to saturations in the gradient descent steps.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_gradient(x):\n",
    "    return np.exp(x) / (1 + np.exp(x)) ** 2\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, sigmoid(x), color=\"purple\")\n",
    "ax.plot(x, sigmoid_gradient(x), color=\"purple\", ls=\"--\")\n",
    "ax.spines[\"left\"].set_position((\"data\", 0.0))\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0.0))\n",
    "\n",
    "ax.set_title(\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tangens hyperbolicus \n",
    "\n",
    "$$\n",
    "g(x) = \\tanh{x}\n",
    "$$\n",
    "\n",
    "The tangens hyperbolicus function behaves very similar to the sigmoid function. Its just a scaled version of it.\n",
    "\n",
    "\n",
    "$$\n",
    "\\rm{d}g(x) = 1 - \\tanh^2{x}\n",
    "$$\n",
    "\n",
    "The gradient also becomes zero for large values of $x$.  However the values of the $\\tanh$ function are centered around zero which has some advantagous properties when compared to the sigmoid function.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_gradient(x):\n",
    "    return 1 - tanh(x) ** 2\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, tanh(x), color=\"purple\")\n",
    "ax.plot(x, tanh_gradient(x), color=\"purple\", ls=\"--\")\n",
    "ax.spines[\"left\"].set_position((\"data\", 0.0))\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0.0))\n",
    "\n",
    "ax.set_title(\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rectified Linear Unit (ReLU)\n",
    "\n",
    "$$\n",
    "g(x) = \\max{0, x}\n",
    "$$\n",
    "\n",
    "The ReLU function became very popular recently. It was used in some very image recognition networks. Evaluating the function is cheap and it often increases the speed of the gradient descent. See [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\n",
    "\n",
    "\n",
    "$$\n",
    "\\rm{d}g(x) = \\begin{cases}\n",
    "                1,  & \\text{if $x > 0$} \\\\\n",
    "                \\text{undefined},  & \\text{if $x = 0$} \\\\\n",
    "                \\text{0},  & \\text{if $x < 0$} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The gradient is zero for value smaller than 0 and is discontinous. This can lead to a problem called 'dying neurons'.\n",
    "The softplus function is a common alternative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_gradient(x):\n",
    "    return np.clip(np.sign(x), 0, 1)\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "\n",
    "def softplus_gradient(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, relu(x), label=\"relu\", color=\"purple\")\n",
    "ax.plot(x, relu_gradient(x), label=\"relu gradient\", color=\"darkred\", ls=\"--\")\n",
    "ax.plot(x, softplus(x), label=\"softplus\", color=\"darkorange\")\n",
    "ax.plot(x, softplus_gradient(x), label=\"softplus gradient\", color=\"darkorange\", ls=\"--\")\n",
    "ax.spines[\"left\"].set_position((\"data\", 0.0))\n",
    "ax.spines[\"bottom\"].set_position((\"data\", 0.0))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "We've seen how easily neural networks can overfit. We can try to avoid that using two techniques.\n",
    "\n",
    "- Add a penalty term to the loss function.\n",
    "- Add special regularization layers.\n",
    "\n",
    "Adapting the loss function is a common method to avoid overfitting not only for neural networks. We have seen a similar idea for the SVM loss function.\n",
    "\n",
    "A common approach is the  __L2__ regularization.\n",
    "\n",
    "$$\n",
    "L_r = L + \\frac{1}{2} \\lambda w^2\n",
    "$$\n",
    "\n",
    "It avoids strong changes in $w$ and creates more diffuse weight matrices.\n",
    "\n",
    "So called __dropout layers__ randomly select neurons to activate during the training. Each iteration only optimizes a subset of neurons.\n",
    "\n",
    "#### Convolutional Neural Nets for Image Recognition\n",
    "\n",
    "The basic idea behind CNNs are simplifications made for handlin high dimensional inputs like high resolution images. \n",
    "Before the first fully connected layers are trained on the input data a so called convolution kernel is applied to the image. The parameters of this kernel are free to be optimized during training. The weights for each region in the image are shared however. This reduces the amount of parameters need to train a network and also leads to __translational invariance__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn.gif](ml/images/cnn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Networks in practice\n",
    "A few key points to take away:\n",
    "\n",
    " - The more layers the network the more powerful it becomes. However training might not converge or overfitting might occur.\n",
    " \n",
    " - The term neural network is a misnomer. That's not how real brains work.\n",
    " \n",
    " - You need regularization to avoid overfitting.\n",
    " \n",
    " - The word *Deep* in *Deep Learning* refers to the number of hidden layers. No magic involved but a lot of hype.\n",
    " \n",
    " - Do not use the sigmoid activation. Use ReLU if you are unsure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software\n",
    "\n",
    "Large networks require considerable computing power to optimize. Matrix computations are well suited for GPUs. The scikit-learn library does not support GPU computation out of the box. It also does not support convoluted neural networks.\n",
    "\n",
    "There is a whole zoo of software libraries to train large neural networks. Here are some important ones.\n",
    "\n",
    "##### Tensorflow\n",
    "\n",
    "Developed by Google Inc. Maybe the most popular solution.\n",
    "\n",
    "![graph_vis_animation.gif](./ml/images/graph_vis_animation.gif)\n",
    "\n",
    "\n",
    "##### Keras\n",
    "\n",
    "High-Level API build ontop of TF\n",
    "\n",
    "<img width=\"50%\" src=\"./ml/images/keras-logo-2018-large-1200.png\">\n",
    "\n",
    "###### pytorsch\n",
    "\n",
    "Facebook's solution \n",
    "https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    '<iframe width=\"660\" height=\"415\" src=\"https://www.youtube.com/embed/cf0JG0791P4\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
